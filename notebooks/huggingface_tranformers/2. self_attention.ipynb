{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled dot-product attention\n",
    "\n",
    "There are several ways to implement a self-attention layer, but the most\n",
    "common one is scaled dot-product attention, from the paper introducing\n",
    "the Transformer architecture.3 There are four main steps required to implement this mechanism:\n",
    "\n",
    "\n",
    "* Project each token embedding into three vectors called query, key, and value.\n",
    "\n",
    "\n",
    "* Compute attention scores. We determine how much the query and key\n",
    "vectors relate to each other using a similarity function. As the name\n",
    "suggests, the similarity function for scaled dot-product attention is\n",
    "the dot product, computed efficiently using matrix multiplication of the\n",
    "embeddings. Queries and keys that are similar will have a large\n",
    "dot product, while those that don’t share much in common\n",
    "will have little to no overlap. The outputs from this step are called\n",
    "the attention scores, and for a sequence with n input\n",
    "tokens there is a corresponding n × n matrix of attention scores.\n",
    "\n",
    "\n",
    "* Compute attention weights. Dot products can in general produce\n",
    "arbitrarily large numbers, which can destabilize the training process.\n",
    "To handle this, the attention scores are first multiplied by a scaling\n",
    "factor to normalize their variance and then normalized with a softmax to\n",
    "ensure all the column values[…]\n",
    "\n",
    "* “Update the token embeddings. Once the attention weights are computed,\n",
    "we multiply them by the value vector to obtain an updated representation for embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2051, 10029,  2066,  2019,  8612]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "model_ckpt = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "text = \"time flies like an arrow\"\n",
    "# Exclude CLS and SEP special tokens to keep it simple\n",
    "inputs = tokenizer(text, return_tensors='pt', add_special_tokens=False)\n",
    "inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30522, 768)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Dense Embeddings\n",
    "import torch\n",
    "from torch import nn \n",
    "from transformers import AutoConfig\n",
    "\n",
    "# Get te config.json file associated with 'bert-base-uncased' \n",
    "config = AutoConfig.from_pretrained(model_ckpt)\n",
    "token_emb = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "token_emb # size: (bert unique_ids x bert embedding_hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the embeddings \n",
    "input_embeds = token_emb(inputs.input_ids)\n",
    "input_embeds.size() # size: (batch_size x seq_len x hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 5])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import sqrt\n",
    "\n",
    "# Create query, key and value vectors \n",
    "query = key = value = input_embeds\n",
    "dim_k = key.size(-1)\n",
    "\n",
    "# Create a 5x5 matrix of attention per sample in the batch \n",
    "scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)\n",
    "scores.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F \n",
    "weights = F.softmax(scores, dim=-1)\n",
    "weights.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiply the attention weights by the values\n",
    "attn_outputs = torch.bmm(weights, value)\n",
    "attn_outputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value): \n",
    "    dim_k = query.size(-1)\n",
    "    scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k) \n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    return torch.bmm(weights, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head Attention\n",
    "\n",
    "In our simple example, we only used the embeddings “as is” to compute\n",
    "the attention scores and weights, but that’s far from the\n",
    "whole story. In practice, the self-attention layer applies three\n",
    "independent linear transformations to each embedding to generate the\n",
    "query, key, and value vectors. These transformations project the\n",
    "embeddings and each projection carries its own set of learnable\n",
    "parameters, which allows the self-attention layer to focus on different\n",
    "semantic aspects of the sequence.\n",
    "\n",
    "It also turns out to be beneficial to have multiple sets of linear\n",
    "projections, each one representing a so-called attention head. The\n",
    "resulting multi-head attention layer is illustrated in\n",
    "Figure 3-5. But why do we need more than one\n",
    "attention head? The reason is that the softmax of one head tends to\n",
    "focus on mostly one aspect of similarity. Having several heads allows\n",
    "the model to focus on several aspects at once. For instance, one head\n",
    "can focus on subject-verb interaction, whereas another finds nearby\n",
    "adjectives. Obviously we don’t handcraft these relations\n",
    "into the model, and they are fully learned from the data. If you are\n",
    "familiar with computer vision models you might see the resemblance to filters in\n",
    "convolutional neural networks, where one filter can be responsible for\n",
    "detecting faces and another one finds wheels of cars in images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module): \n",
    "    def __init__(self, embed_dim, head_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(embed_dim, head_dim)\n",
    "        self.k = nn.Linear(embed_dim, head_dim)\n",
    "        self.v = nn.Linear(embed_dim, head_dim)\n",
    "\n",
    "    def forward(self, hidden_state) -> torch.Tensor: \n",
    "        return scaled_dot_product_attention(\n",
    "            self.q(hidden_state), \n",
    "            self.k(hidden_state), \n",
    "            self.v(hidden_state)\n",
    "        )\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module): \n",
    "    def __init__(self, config: dict) -> None:\n",
    "        \"\"\"\n",
    "        config: BERT from tranformers config.json file \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        embed_dim = config.hidden_size             # 768 \n",
    "        num_heads = config.num_attention_heads     # 12 \n",
    "        head_dim = embed_dim // num_heads          # 64 \n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(embed_dim, head_dim) \n",
    "                for _ in range(num_heads)]\n",
    "        )\n",
    "        self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, hidden_state): \n",
    "        x = torch.cat([h(hidden_state) for h in self.heads], dim=-1)\n",
    "        return self.output_linear(x) # size: (batch_size, seq_len, hidden_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multihead_attn = MultiHeadAttention(config)\n",
    "attn_output = multihead_attn(input_embeds)\n",
    "attn_output.size()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Feed-Forward Layer\n",
    "\n",
    "The feed-forward sublayer in the encoder and decoder is just a simple\n",
    "two-layer fully connected neural network, but with a twist: instead of\n",
    "processing the whole sequence of embeddings as a single vector, it\n",
    "processes each embedding independently. For this reason, this layer is\n",
    "often referred to as a position-wise feed-forward layer. You may also\n",
    "see it referred to as a one-dimensional convolution with a kernel size\n",
    "of one, typically by people with a computer vision background (e.g., the\n",
    "OpenAI GPT codebase uses this nomenclature). A rule of thumb from the\n",
    "literature is for the hidden size of the first layer to be four times\n",
    "the size of the embeddings, and a GELU activation function is most\n",
    "commonly used. This is where most of the capacity and memorization is\n",
    "hypothesized to happen, and it’s the part that is most often\n",
    "scaled when scaling up the models. We can implement this as a simple\n",
    "nn.Module as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config: dict) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.intermediate_size),\n",
    "            nn.Linear(config.intermediate_size, config.hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(config.hidden_dropout_prob)\n",
    "        )\n",
    "\n",
    "    def forward(self, x): \n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_forward = FeedForward(config)\n",
    "ff_outputs = feed_forward(attn_output)\n",
    "ff_outputs.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Layer Normalization\n",
    "\n",
    "As mentioned earlier, the Transformer architecture makes use of layer\n",
    "normalization and skip connections. The former normalizes each input\n",
    "in the batch to have zero mean and unity variance. Skip connections pass\n",
    "a tensor to the next layer of the model without processing and add it to\n",
    "the processed tensor. When it comes to placing the layer normalization\n",
    "in the encoder or decoder layers of a transformer, there are two main\n",
    "choices adopted in the literature:\n",
    "\n",
    "##### Post layer normalization\n",
    "\n",
    "  This is the arrangement used in the\n",
    "Transformer paper; it places layer normalization in between the skip\n",
    "connections. This arrangement is tricky to train from scratch as the\n",
    "gradients can diverge. For this reason, you will often see a concept\n",
    "known as learning rate warm-up, where the learning rate is gradually\n",
    "increased from a small value to some maximum value during\n",
    "training.\n",
    "\n",
    "##### Pre layer normalization\n",
    "\n",
    "  This is the most common arrangement\n",
    "found in the literature; it places layer normalization within the span\n",
    "of the skip connections. This tends to be much more stable during\n",
    "training, and it does not usually require any learning rate\n",
    "warm-up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, config: dict) -> None:\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.feed_forward = FeedForward(config)\n",
    "\n",
    "    def forward(self, x): \n",
    "        # Apply layer normalization and then copy input into query, key and value\n",
    "        hidden_state = self.layer_norm_1(x)\n",
    "        # Apply attention with a skeep connection \n",
    "        x = x + self.attention(hidden_state)\n",
    "        #Apply feed-forward layer with a skip connection \n",
    "        x = x + self.feed_forward(self.layer_norm_2(x))\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 5, 768]), torch.Size([1, 5, 768]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_layer = TransformerEncoderLayer(config)\n",
    "input_embeds.shape, encoded_layer(input_embeds).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b7cf96f98d7696330d6b15e593b47aaec2e84baf3d164bd5b6c17e57a28d15c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
