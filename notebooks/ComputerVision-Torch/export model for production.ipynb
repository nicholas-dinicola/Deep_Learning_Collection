{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap Your Model for Inference\n",
    "Exporting a model for production means packaging your model in a stand-alone format that can be transferred and used to perform inference in a production environment, such as an API or a website.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production-Ready Preprocessing\n",
    "\n",
    "Remember that the images need some preprocessing before being fed to the CNN. For example, typically you need to resize, center crop, and normalize the image with a transform pipeline similar to this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T \n",
    "\n",
    "testval_transforms = T.Compose(\n",
    "    [\n",
    "        # The size here depends on your application. Here let's use 256x256\n",
    "        T.Resize(256),\n",
    "        # Let's take the central 224x224 part of the image\n",
    "        T.CenterCrop(224),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, if you do not do these operations in production the performance of your model is going to suffer greatly.\n",
    "\n",
    "The best course of action is to make these transformations part of your standalone package instead of re-implementing them in the production environment. Let's see how.\n",
    "\n",
    "We need to wrap our model in a wrapper class that is going to take care of applying the transformations and then run the transformed image through the CNN.\n",
    "\n",
    "If we trained with the **nn.CrossEntropyLoss** as the loss function, we also need to apply a softmax function to the output of the model so that the output of the wrapper will be probabilities and not merely scores.\n",
    "\n",
    "Let's see an example of such a wrapper class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as T\n",
    "from __future__ import annotations\n",
    "\n",
    "\n",
    "class Predictor(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "      self, \n",
    "      model: nn.Module, \n",
    "      class_names: list[str], \n",
    "      mean: torch.Tensor, # mean of the dataset eg. for image dataset (mean_R, mean_G, mean_B) for each collor chanel in tuple \n",
    "      std: torch.Tensor   # std of the dataset eg. for image dataset (stdR, stdG, stdB) for each collor chanel in tuple \n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model.eval()\n",
    "        self.class_names = class_names\n",
    "\n",
    "        self.transforms = nn.Sequential(\n",
    "            T.Resize([256, ]),\n",
    "            T.CenterCrop(224),\n",
    "            T.ConvertImageDtype(torch.float),\n",
    "            T.Normalize(mean.tolist(), std.tolist())\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            # 1. apply transforms\n",
    "            x = self.transforms(x)  # =\n",
    "            # 2. get the logits\n",
    "            x = self.model(x)  # =\n",
    "            # 3. apply softmax\n",
    "            #    HINT: remmeber to apply softmax across dim=1\n",
    "            x = F.softmax(x, dim=1)  # =\n",
    "\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This defines the transformations we want to apply. It looks very similar to the transform validation pipeline, with a few important differences:\n",
    "\n",
    "* We do not use nn.Compose but nn.Sequential. Indeed the former is not supported by torch.script (the export functionality of PyTorch).\n",
    "* In Resize the size specification must be a tuple or a list, and not a scalar as we were able to do during training.\n",
    "* There is no ToTensor. Instead, we use T.ConvertImageDtype. Indeed, in this context the input to the forward method is going to be already a Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Using torchscript\n",
    "We can now create an instance of our Predictor wrapper and save it to file using torch.script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyMOdel(*args, **kwargs)\n",
    "predictor = Predictor(model, class_names, mean, std).cpu()\n",
    "\n",
    "# Export using torch.jit.script\n",
    "scripted_predictor = torch.jit.script(predictor)\n",
    "scripted_predictor.save(\"standalone_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we move the Predictor instance to the CPU before exporting it. When reloading the model, the model will be loaded on the device it was taken from. So if we want to do inference on the CPU, we need to first move the model there. In many cases CPUs are enough for inference, and they are much cheaper than GPUs.\n",
    "\n",
    "We then use torch.jit.script which converts our wrapped model into an intermediate format that can be saved to disk (which we do immediately after).\n",
    "\n",
    "Now, in a different process or a different computer altogether, we can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "predictor_reloaded = torch.jit.load(\"standalone_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will recreate our wrapped model. We can then use it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Reload the model\n",
    "learn_inf = torch.jit.load(\"checkpoints/transfer_exported.pt\")\n",
    "\n",
    "# Read an image and transform it to tensor to simulate what would\n",
    "# happen in production\n",
    "img = Image.open(\"static_images/test/09.Golden_Gate_Bridge/190f3bae17c32c37.jpg\")\n",
    "# We use .unsqueeze because the model expects a batch, so this\n",
    "# creates a batch of 1 element\n",
    "t = T.ToTensor()(img).unsqueeze_(0)\n",
    "\n",
    "# Perform inference and get the softmax vector\n",
    "softmax = learn_inf(pil_to_tensor).squeeze()\n",
    "# Get index of the winning label\n",
    "max_idx = softmax.argmax()\n",
    "# Print winning label using the class_names attribute of the \n",
    "# model wrapper\n",
    "print(f\"Prediction: {learn_inf.class_names[max_idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE that there are 2 different formats that can be used to export a model: script and trace. Scripting is more general, but in some cases you do have to use tracing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9e778bcbc61423a83587c3c637e8b20c47225974837f34832d4e92a25528a275"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
